{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Collections and Queries in DataFed\n",
    "In this notebook, we will be going over creating Collections, viewing contained items, organizing Collections, downloading Collections, and searching for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we begin:\n",
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from datafed.CommandLib import API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the DataFed API and set ``context`` to the Training project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api = API()\n",
    "df_api.setContext(\"p/trn001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Reset this variable to your username or Globus ID so that you work within your own collection by default </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_collection = \"somnaths\"  # Name of this user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "In this notebook, let us assume that we are working on a machine learning problem aimed at putting together training data for a machine learning model. For illustration purposes, we will assume that we aim to train a classifier for classifying animals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Collection\n",
    "First, let us create a collection to hold all our data. \n",
    "\n",
    "We will be using the ``collectionCreate()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(coll {\n",
      "  id: \"c/43657980\"\n",
      "  title: \"Image classification training data\"\n",
      "  owner: \"p/trn001\"\n",
      "  ct: 1615405612\n",
      "  ut: 1615405612\n",
      "  parent_id: \"c/34558900\"\n",
      "}\n",
      ", 'CollDataReply')\n"
     ]
    }
   ],
   "source": [
    "coll_resp = df_api.collectionCreate(\n",
    "    \"Image classification training data\", parent_id=parent_collection\n",
    ")\n",
    "print(coll_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we got back a ``CollDataReply`` object. This is somewhat similar to what you get from ``dataCreate()`` we just saw. \n",
    "\n",
    "Now, let's Extract the ``id`` of this newly created collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c/43657980\n"
     ]
    }
   ],
   "source": [
    "train_coll_id = coll_resp[0].coll[0].id\n",
    "print(train_coll_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate with training data\n",
    "Now that we have a place to put the training data, let us populate this collection with examples of animals\n",
    "### Define a function to generate (fake) training data:\n",
    "We need a function to:\n",
    "* Create a Data Record\n",
    "* Put data into this Data Record\n",
    "\n",
    "For simplicity we will use some dummy data from a public Globus Endpoint This information has been filled in for you via the ``raw_data_path`` variable. \n",
    "\n",
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> We have a skeleton function prepared for you along with comments to guide you. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animal_data(is_dog=True):\n",
    "    this_animal = \"cat\"\n",
    "    if is_dog:\n",
    "        this_animal = \"dog\"\n",
    "    # Ideally, we would have more sensible titles such as \"Doberman\",\n",
    "    # \"Poodle\", etc. instead of \"Dog_234\"\n",
    "    # To mimic a real-life scenario, we append a number to the animal\n",
    "    # type to denote the N-th example of a cat or dog. In this case,\n",
    "    # we use a random integer.\n",
    "    title = this_animal + \"_\" + str(random.randint(1, 1000))\n",
    "    # Create the record here:\n",
    "    rec_resp = df_api.dataCreate(\n",
    "        title,\n",
    "        metadata=json.dumps({\"animal\": this_animal}),\n",
    "        parent_id=train_coll_id,\n",
    "    )\n",
    "\n",
    "    # Extract the ID of the Record:\n",
    "    this_rec_id = rec_resp[0].data[0].id\n",
    "\n",
    "    # path to the file containing the (dummy) raw data\n",
    "    raw_data_path = (\n",
    "        \"sdss#public/uufs/chpc.utah.edu/common/home/sdss/dr10/\"\n",
    "        \"apogee/spectro/data/55574/55574.md5sum\"\n",
    "    )\n",
    "\n",
    "    # Put the raw data into the record you just created:\n",
    "    put_resp = df_api.dataPut(this_rec_id, raw_data_path)\n",
    "\n",
    "    # Only return the ID of the Data Record you created:\n",
    "    return this_rec_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 5 examples of cats and dogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_records = list()\n",
    "dog_records = list()\n",
    "for _ in range(5):\n",
    "    dog_records.append(generate_animal_data(is_dog=True))\n",
    "for _ in range(5):\n",
    "    cat_records.append(generate_animal_data(is_dog=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d/43659103', 'd/43659126', 'd/43659149', 'd/43659172', 'd/43659195']\n"
     ]
    }
   ],
   "source": [
    "print(cat_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d/43658988', 'd/43659011', 'd/43659034', 'd/43659057', 'd/43659080']\n"
     ]
    }
   ],
   "source": [
    "print(dog_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing items in a Collection:\n",
    "Let us take a look at the training data we have assembled so far using the ``colectionItemsList()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(item {\n",
      "  id: \"d/43658988\"\n",
      "  title: \"dog_196\"\n",
      "  owner: \"p/trn001\"\n",
      "  creator: \"u/somnaths\"\n",
      "  size: 2842.0\n",
      "  notes: 0\n",
      "}\n",
      "item {\n",
      "  id: \"d/43659034\"\n",
      "  title: \"dog_57\"\n",
      "  owner: \"p/trn001\"\n",
      "  creator: \"u/somnaths\"\n",
      "  size: 2842.0\n",
      "  notes: 0\n",
      "}\n",
      "item {\n",
      "  id: \"d/43659080\"\n",
      "  title: \"dog_686\"\n",
      "  owner: \"p/trn001\"\n",
      "  creator: \"u/somnaths\"\n",
      "  size: 2842.0\n",
      "  notes: 0\n",
      "}\n",
      "item {\n",
      "  id: \"d/43659011\"\n",
      "  title: \"dog_689\"\n",
      "  owner: \"p/trn001\"\n",
      "  creator: \"u/somnaths\"\n",
      "  size: 2842.0\n",
      "  notes: 0\n",
      "}\n",
      "item {\n",
      "  id: \"d/43659057\"\n",
      "  title: \"dog_778\"\n",
      "  owner: \"p/trn001\"\n",
      "  creator: \"u/somnaths\"\n",
      "  size: 2842.0\n",
      "  notes: 0\n",
      "}\n",
      "offset: 5\n",
      "count: 20\n",
      "total: 10\n",
      ", 'ListingReply')\n"
     ]
    }
   ],
   "source": [
    "coll_list_resp = df_api.collectionItemsList(train_coll_id, offset=5)\n",
    "print(coll_list_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Note </span>\n",
    "> <span style=\"color:blue\"> If we had several dozens, hundreds, or even thousands of items in a Collection, we would need to call ``collectionItemsList()`` multiple times by stepping up the ``offset`` keyword argument each time to get the next “page” of results. </span>\n",
    "\n",
    "### <span style=\"color:green\"> Discussion </span>\n",
    "<span style=\"color:green\"> Let's say that we are only interested in finding records that have cats in this (potentially) large collection of training data. How do we go about doing that? </span>\n",
    "\n",
    "# Data Query / Search\n",
    "### <span style=\"color:red\"> Caution </span>\n",
    "> <span style=\"color:red\"> Search vocabulary is likely to change with newer versions of DataFed </span>\n",
    "\n",
    "Use the DataFed web interface to:\n",
    "* Search for cats\n",
    "* Specifically in your collection\n",
    "* Save the query\n",
    "\n",
    "### <span style=\"color:blue\"> Note </span>\n",
    "> <span style=\"color:blue\"> Saved queries can be found in the bottom of the navigation (left) pane under ``Project Data`` and ``Saved Queries`` </span>\n",
    "\n",
    "# Find saved queries:\n",
    "We can list all saved queries via ``queryList()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(item {\n",
      "  id: \"q/43664114\"\n",
      "  title: \"is_cat\"\n",
      "}\n",
      "offset: 0\n",
      "count: 20\n",
      "total: 1\n",
      ", 'ListingReply')\n"
     ]
    }
   ],
   "source": [
    "ql_resp = df_api.queryList()\n",
    "print(ql_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we again recieved the familiar ``ListingReply`` object as the response\n",
    "\n",
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Get the ``id`` of the desired query out of the response: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q/43664114\n"
     ]
    }
   ],
   "source": [
    "query_id = ql_resp[0].item[0].id\n",
    "print(query_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the saved query\n",
    "Use the ``queryView()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(query {\n",
       "   id: \"q/43664114\"\n",
       "   title: \"is_cat\"\n",
       "   query: \"{\\\"meta\\\":\\\"md.animal == \\\\\\\"cat\\\\\\\"\\\",\\\"scopes\\\":[{\\\"scope\\\":4,\\\"id\\\":\\\"c/43657980\\\",\\\"recurse\\\":true}]}\"\n",
       "   owner: \"u/somnaths\"\n",
       "   ct: 1615406400\n",
       "   ut: 1615406400\n",
       " },\n",
       " 'QueryDataReply')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_api.queryView(query_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the saved query\n",
    "Use the ``queryExec()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(item {\n",
      "  id: \"d/43659103\"\n",
      "  title: \"cat_209\"\n",
      "  owner: \"p/trn001\"\n",
      "  creator: \"u/somnaths\"\n",
      "  size: 2842.0\n",
      "  notes: 0\n",
      "}\n",
      "item {\n",
      "  id: \"d/43659126\"\n",
      "  title: \"cat_511\"\n",
      "  owner: \"p/trn001\"\n",
      "  creator: \"u/somnaths\"\n",
      "  size: 2842.0\n",
      "  notes: 0\n",
      "}\n",
      "item {\n",
      "  id: \"d/43659149\"\n",
      "  title: \"cat_341\"\n",
      "  owner: \"p/trn001\"\n",
      "  creator: \"u/somnaths\"\n",
      "  size: 2842.0\n",
      "  notes: 0\n",
      "}\n",
      "item {\n",
      "  id: \"d/43659172\"\n",
      "  title: \"cat_558\"\n",
      "  owner: \"p/trn001\"\n",
      "  creator: \"u/somnaths\"\n",
      "  size: 2842.0\n",
      "  notes: 0\n",
      "}\n",
      "item {\n",
      "  id: \"d/43659195\"\n",
      "  title: \"cat_821\"\n",
      "  owner: \"p/trn001\"\n",
      "  creator: \"u/somnaths\"\n",
      "  size: 2842.0\n",
      "  notes: 0\n",
      "}\n",
      ", \"ListingReply\")\n"
     ]
    }
   ],
   "source": [
    "query_resp = df_api.queryExec(query_id)\n",
    "print(query_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet again, we get back the ``ListingReply`` message. \n",
    "\n",
    "Now let us extract just the ``id``s from each of the items in the message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d/43659103', 'd/43659126', 'd/43659149', 'd/43659172', 'd/43659195']\n"
     ]
    }
   ],
   "source": [
    "cat_rec_ids = list()\n",
    "for record in query_resp[0].item:\n",
    "    cat_rec_ids.append(record.id)\n",
    "# one could also use list comprehensions to get the answer in one line:\n",
    "# cat_rec_ids = [record.id for record in query_resp[0].item]\n",
    "print(cat_rec_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have the ground truth in ``cat_records``. Is this the same as what we got from the query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(set(cat_rec_ids) == set(cat_records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating cats from dogs\n",
    "Our goal now is to gather all cat Data Records into a dedicated Collection\n",
    "\n",
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Create a new collection to hold the Cats record </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c/43666045\n"
     ]
    }
   ],
   "source": [
    "coll_resp = df_api.collectionCreate(\"Cats\", parent_id=train_coll_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Extract the ``id`` for this Collection: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c/43666045\n"
     ]
    }
   ],
   "source": [
    "cat_coll_id = coll_resp[0].coll[0].id\n",
    "print(cat_coll_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Items to Collection\n",
    "Now let us add only the cat Data Records into this new collection using the ``collectionItemsUpdate()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(, 'ListingReply')\n"
     ]
    }
   ],
   "source": [
    "cup_resp = df_api.collectionItemsUpdate(cat_coll_id, add_ids=cat_rec_ids)\n",
    "print(cup_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike most DataFed functions, this function doesn't really return much\n",
    "\n",
    "Now, let us view the contents of the Cats Collection to make sure that all Cat Data Records are present in this Collection. \n",
    "\n",
    "Just to keep the output clean and short, we will only extract the ID and title of the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d/43659103 cat_209\n",
      "d/43659149 cat_341\n",
      "d/43659126 cat_511\n",
      "d/43659172 cat_558\n",
      "d/43659195 cat_821\n"
     ]
    }
   ],
   "source": [
    "ls_resp = df_api.collectionItemsList(cat_coll_id)\n",
    "# Iterating through the items in the Collection and only extracting\n",
    "# a few items:\n",
    "for obj in ls_resp[0].item:\n",
    "    print(obj.id, obj.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> View the contents of the main training data Collection. <br> You may use the snippet above if you like and modify it accordingly </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_resp = df_api.collectionItemsList(train_coll_id)\n",
    "# Iterating through the items in the Collection and only extracting a\n",
    "# few items:\n",
    "for obj in ls_resp[0].item:\n",
    "    print(obj.id, obj.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Note </span>\n",
    "> <span style=\"color:blue\"> Data Records can exist in **multiple** Collections just like video or songs can exist on multiple playlists </span>\n",
    "\n",
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Remove the cat Data Records from the training data collection. They already exist in the \"Cats\" Collection. <br> **Hint**: The function call is very similar to the function call for adding cats to the \"Cats\" collection </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(, 'ListingReply')\n"
     ]
    }
   ],
   "source": [
    "cup_resp = df_api.collectionItemsUpdate(train_coll_id, rem_ids=cat_rec_ids)\n",
    "print(cup_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> View the contents of the training data Collection. <br> You may reuse a code snippet from an earlier cell. <br> Do you see the individual cat Data Records in this collection? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_resp = df_api.collectionItemsList(train_coll_id)\n",
    "# Iterating through the items in the Collection and only extracting\n",
    "# a few items:\n",
    "for obj in ls_resp[0].item:\n",
    "    print(obj.id, obj.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search or Organize?\n",
    "If you could always search for your data, what is the benefit to organizing them into collections?\n",
    "\n",
    "# Download entire Collection\n",
    "\n",
    "### <span style=\"color:blue\"> Note </span>\n",
    "> <span style=\"color:blue\"> Recall that DataFed can download arbitrarily large number of Records regardless of the physical locations of the DataFed repositories containing the data. </span>\n",
    "\n",
    "Let us first make sure we don't already have a directory with the desired name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = \"./cat_data\"\n",
    "\n",
    "if os.path.exists(dest_dir):\n",
    "    import shutil\n",
    "\n",
    "    shutil.rmtree(dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Download the entire Cat Collection with a single DataFed function call. <br> **Hint:** You may want to look at a function we used in the third notebook </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(task {\n",
       "   id: \"task/43667334\"\n",
       "   type: TT_DATA_GET\n",
       "   status: TS_SUCCEEDED\n",
       "   client: \"u/somnaths\"\n",
       "   step: 2\n",
       "   steps: 3\n",
       "   msg: \"Finished\"\n",
       "   ct: 1615407332\n",
       "   ut: 1615407338\n",
       "   source: \"d/43659103, d/43659126, d/43659149, d/43659172, d/43659195, ...\"\n",
       "   dest: \"1646e89e-f4f0-11e9-9944-0a8c187e8c12/Users/syz/OneDrive - Oak Ridge National Laboratory/DataFed_Tutorial/Notebooks/cat_data\"\n",
       " },\n",
       " 'TaskDataReply')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_api.dataGet(cat_coll_id, \"./cat_data\", wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that we did infact download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['43659195.md5sum',\n",
       " '43659103.md5sum',\n",
       " '43659172.md5sum',\n",
       " '43659126.md5sum',\n",
       " '43659149.md5sum']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Optional Exercise </span>\n",
    "<span style=\"color:green\">1. Create a new Collection to hold the simulation data you created in the previous notebook <br>2. Use the functions you saw above to ensure that the Data Records only exist in the Simulation Collection </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
