{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transferring Data in DataFed\n",
    "In this notebook, we will be going over uploading to & downloading from Data Records and monitoring data transfer tasks.\n",
    "\n",
    "## Before we begin:\n",
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datafed.CommandLib import API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the DataFed API and set ``context`` to the Training project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api = API()\n",
    "df_api.setContext(\"p/trn001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Enter your username to work within your personal Collection. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_collection = \"c/34558900\"  # your username here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Copy over the ID for the record you created in the previous notebook. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_id = \"d/43650517\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading raw data\n",
    "Shortly, we will learn how to upload data to the record we just created. For demonstration purposes, we will just create a simple text file and use this as the raw data for the Data Record\n",
    "\n",
    "### <span style=\"color:blue\"> **Note:** </span>\n",
    "> <span style=\"color:blue\"> DataFed does not impose any restrictions on the file extension / format for the raw data </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./raw_data.txt\", mode=\"w\") as file_handle:\n",
    "    file_handle.write(\"This is some data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Note </span>\n",
    "> <span style=\"color:blue\"> Always ensure that your Globus endpoint is active and that your files are located in a directory that is visible to the Globus Endpoint </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `dataPut()`\n",
    "Uploading data files to DataFed is done using the `dataPut` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put_resp = df_api.dataPut(\n",
    "    record_id,\n",
    "    \"./raw_data.txt\",\n",
    "    wait=True,  # Waits until transfer completes.\n",
    ")\n",
    "print(put_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get two components in the response:\n",
    "* Information about the Data Record, data was uploaded to\n",
    "* Information about the data transfer ``task`` - more on this later\n",
    "\n",
    "The ``dataPut()`` method **initiates a Globus transfer** on our behalf from the machine **wherever** the file was present to wherever the default data repository is located. In this case, the file was in our local file system and on the same machine where we are executing the command.\n",
    "\n",
    "### <span style=\"color:blue\"> Note </span>\n",
    "> <span style=\"color:blue\"> The above data file was specified by its relative local path, so DataFed used our pre-configured default Globus endpoint to find the data file. As long as we have the id for any *active* Globus endpoint that we have authenticated access to, we can transfer data from that endpoint with its full absolute file path â€“ even if the file system is not attached ot the local machine. Look for more information on this in later examples. </span>\n",
    "\n",
    "Let's view the data record now that we've uploaded our data. Pay attention to the ``ext`` and ``source`` fields which should now populated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_resp = df_api.dataView(record_id)\n",
    "print(dv_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading raw data\n",
    "DataFed is also capable of getting data stored in a DataFed repository and placing it in the local or other Globus-visible filesystem via the ``dataGet()`` function. \n",
    "\n",
    "Let us download the content in the data record we have been working on so far for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_resp = df_api.dataGet(\n",
    "    record_id,\n",
    "    \".\",  # directory where data should be downloaded\n",
    "    orig_fname=False,  # do not name file by its original name\n",
    "    wait=True,  # Wait until Globus transfer completes\n",
    ")\n",
    "print(get_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the response we only get back information about the data transfer ``task`` - more on this shortly\n",
    "\n",
    "The bug in ``dataGet()`` also reveals its capability to **download multiple data records or even Collections.**\n",
    "\n",
    "Let's confirm that the data has been downloaded successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_file_name = os.path.join(\".\", record_id.split(\"d/\")[-1]) + \".txt\"\n",
    "print(\"Does a file with this name: \" f\"{expected_file_name} exist?\")\n",
    "print(os.path.exists(expected_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "### <span style=\"color:blue\"> Note </span>\n",
    "> <span style=\"color:blue\"> A DataFed task may itself contain / be responsible for several Globus file transfers, potentially from / to multiple locations. </span>\n",
    "    \n",
    "DataFed makes it possible to check on the status of transfer tasks in an easy and programmatic manner.\n",
    "\n",
    "Before we learn more about tasks, first lets try to get the ``id`` of the task in ``get_resp`` from the recent ``dataGet()`` function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = get_resp[0].task[0].id\n",
    "print(task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Tasks\n",
    "We can get more information about a given transfer via the `taskView()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_resp = df_api.taskView(task_id)\n",
    "print(task_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a new kind of message - a ``TaskDataReply``. \n",
    "Key fields to keep an eye on:\n",
    "* ``status``\n",
    "* ``msg``\n",
    "* ``source``\n",
    "* ``dest``\n",
    "\n",
    "If we are interested in monitoring tasks, triggering activities or subsequent steps of workflows based on transfers, we would need to know how to get the ``status`` property from the ``TaskDataReply``: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_resp[0].task[0].status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the message above says `TS_SUCCEEDED`, we see that this task status codes to the integer `3`.\n",
    "\n",
    "### <span style=\"color:blue\"> **Note:** </span>\n",
    "> <span style=\"color:blue\"> Cheat sheet for interpreting task statuses: <br> ``2``: in progress <br> ``3``: complete <br> anything else - problem  </span>\n",
    "\n",
    "### <span style=\"color:blue\"> **Note:** </span>\n",
    "> <span style=\"color:blue\"> A future version of DataFed may change the nature of the output / type for the status property. In general, the exact return object types and nomenclature may evolve with DataFed. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Tasks\n",
    "We can request a listing of all our recently initiated tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_api.taskList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this listing would be very helpful for the Exercise below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example scenario - Simulations\n",
    "Let's say that we want to run a series of simulations where one or more parameters are varied and each simulation is run with a unique set of parameters. Let's also assume that our eventual goal is to build a surrogate model for the computationally expensive simulation using machine learning. So, we **want to capture the metadata and data associated with the series of simulations** to train the machine learning model later on. \n",
    "\n",
    "We have set up skeleton functions and code snippets to help you mimic the data management for such a simulation. We would like you to take what you have learnt so far and fill in the blanks\n",
    "<br>\n",
    "\n",
    "\n",
    "### Fake simulation\n",
    "Here, we have simulated a computationally \"expensive\" simulation that simply sleeps for a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expensive_simulation():\n",
    "    time.sleep(5)\n",
    "    # Yes, this simulation is deterministic and always\n",
    "    # results in the same result:\n",
    "    path_to_results = \"sdss#public/uufs/chpc.utah.edu/common/home/\n",
    "sdss/dr10/apogee/spectro/data/55574/55574.md5sum\"\n",
    "    # The simulation uses the same combination of parameters\n",
    "    metadata = {\"a\": 1, \"b\": 2, \"c\": 3.14}\n",
    "    return path_to_results, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Define a function that: <br> 1. creates a new Data Record with the provided metadata (as a dictionary) and other details, <br> 2. extracts the record id, <br> 3. puts the raw data into the record, <br> 4. extracts and returns the task ID. <br><br> Feel free to print any messages that may help you track things. </span>\n",
    "\n",
    "### <span style=\"color:blue\"> Note </span>\n",
    "> <span style=\"color:blue\"> Pay attention to the ``wait`` keyword argument when putting the raw data into record </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation_index (integer) - counter to signify the Nth simulation in\n",
    "#                              the series\n",
    "# metadata      (dictionary) - combination of parameters used for this\n",
    "#                              simulation\n",
    "# raw_data_path     (string) - Path to the raw data file that needs to\n",
    "#                              be put into the receord\n",
    "# parent_collection (string) - Collection to create this Data Record\n",
    "#                              into\n",
    "def capture_data(\n",
    "    simulation_index,\n",
    "    metadata,\n",
    "    raw_data_path,\n",
    "    parent_collection=parent_collection,\n",
    "):\n",
    "\n",
    "    # 1. Create a new Data Record with the metadata and use the\n",
    "    # simulation index to provide a unique title\n",
    "    rec_resp = df_api.dataCreate(\n",
    "        \"Simulation_\" + str(simulation_index),\n",
    "        metadata=json.dumps(metadata),\n",
    "        parent_id=parent_collection,\n",
    "    )\n",
    "\n",
    "    # 2. Extract the record ID from the response\n",
    "    this_rec_id = rec_resp[0].data[0].id\n",
    "\n",
    "    # 3. Put the raw data into this record:\n",
    "    put_resp = df_api.dataPut(this_rec_id, raw_data_path, wait=False)\n",
    "\n",
    "    # 4. Extract the ID for the data transfer task\n",
    "    task_id = put_resp[0].task.id\n",
    "\n",
    "    # 5. Return the task ID\n",
    "    return task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Try out this function to make sure it works. See what it does on the **DataFed web portal**. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'task/45433451'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_results, metadata = expensive_simulation()\n",
    "\n",
    "task_id = capture_data(14, metadata, path_to_results)\n",
    "task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> We will want a simple function to monitor the status of all the data upload tasks. Define a function that accepts a list of task IDs and returns their status after looking them up on DataFed </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_xfer_status(task_ids):\n",
    "\n",
    "    # put singular task ID into a list\n",
    "    if isinstance(task_ids, str):\n",
    "        task_ids = [task_ids]\n",
    "\n",
    "    # Create a list to hold the statuses of each of the tasks\n",
    "    statuses = list()\n",
    "\n",
    "    # Iterate over each of the task IDs\n",
    "    for this_task_id in task_ids:\n",
    "\n",
    "        # For each task ID, get detailed information about it\n",
    "        task_resp = df_api.taskView(this_task_id)\n",
    "\n",
    "        # Extract the task status from the detailed information\n",
    "        this_status = task_resp[0].task[0].status\n",
    "\n",
    "        # Append this status to the list of statuses\n",
    "        statuses.append(this_status)\n",
    "\n",
    "    # Return the list of statuses\n",
    "    return statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> Try out your function using the IDs of the recent ``dataPut()`` and ``dataGet()`` functions. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_xfer_status(task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the series of simulations:\n",
    "Use the three functions defined above to mimic the process of exploring a parameter space using simulations, where for each iteration, we: <br> 1. run a simulation, <br> 2. capture the data + metadata into DataFed, <br> 3. monitor the data upload tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation #0\n",
      "Transfer status(es): [2]\n",
      "\n",
      "Starting simulation #1\n",
      "Transfer status(es): [2, 2]\n",
      "\n",
      "Starting simulation #2\n",
      "Transfer status(es): [3, 2, 2]\n",
      "\n",
      "Simulations complete! Waiting for uploads to complete\n",
      "\n",
      "Transfer status(es): [3, 3, 2]\n",
      "Transfer status(es): [3, 3, 2]\n",
      "Transfer status(es): [3, 3, 2]\n",
      "Transfer status(es): [3, 3, 2]\n",
      "Transfer status(es): [3, 3, 3]\n",
      "\n",
      "Finished uploading all data!\n"
     ]
    }
   ],
   "source": [
    "xfer_tasks = list()\n",
    "for ind in range(3):\n",
    "    print(\"Starting simulation #{}\".format(ind))\n",
    "    # Run the simulation.\n",
    "    path_to_results, metadata = expensive_simulation()\n",
    "    # Capture the data and metadata into DataFed\n",
    "    task_id = capture_data(ind, metadata, path_to_results)\n",
    "    # Append the task ID for this data upload into xfer_tasks\n",
    "    xfer_tasks.append(task_id)\n",
    "    # Print out the status of the data transfers\n",
    "    print(\"Transfer status(es): {}\".format(check_xfer_status(xfer_tasks)))\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Simulations complete! Waiting for uploads to complete\\n\")\n",
    "\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "    statuses = check_xfer_status(xfer_tasks)\n",
    "    print(\"Transfer status(es): {}\".format(statuses))\n",
    "    if all([this == 3 for this in statuses]):\n",
    "        break\n",
    "\n",
    "print(\"\\nFinished uploading all data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Exercise </span>\n",
    "<span style=\"color:green\"> What happens if you set the ``wait`` in ``dataPut()`` to ``True``? </span>\n",
    "\n",
    "### <span style=\"color:blue\"> **Note:** </span>\n",
    "> <span style=\"color:blue\"> Users are recommended to perform data orchestration (especially large data movement - upload / download) operations outside the scope of heavy / parallel computation operations in order to avoid wasting precious wall time on compute clusters.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
